{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c1c07e-73a8-4f64-ba8e-8a0d81138549",
   "metadata": {},
   "source": [
    "# Tarea. Notebook BoW\n",
    "### Vladimir Garc√≠a Loginova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8065e-2c64-4e32-8f77-941da8fdc837",
   "metadata": {},
   "source": [
    "### Se importan paqueter√≠as y se descargan elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0109f541-eae6-4323-814a-c524cbce183e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vcx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/vcx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy as spc\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nlp = spc.load(\"es_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556994c-8101-41f9-9ee1-84bd55202d56",
   "metadata": {},
   "source": [
    "### Lectura de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46588aa-b031-455f-96ba-fd52a4f21840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"df_mini_HS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361e070-f1f7-41ca-bc85-b2f702c0e617",
   "metadata": {},
   "source": [
    "### Creaci√≥n de listas con oraciones lematizadas y vocabulario de cada texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de33a13-22fb-43d7-807d-96ec6d48c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\:'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\:'\n",
      "/var/folders/fw/lfwqgrwx72v8gqhx3vm2nv780000gn/T/ipykernel_852/4040966600.py:9: SyntaxWarning: invalid escape sequence '\\:'\n",
      "  oracion = re.sub(\"http[s]?\\://\\S+\", \"\", oracion)  # Eliminar enlaces\n"
     ]
    }
   ],
   "source": [
    "all_voc=[]\n",
    "ols=[]\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "\n",
    "    #linmpieza de datos\n",
    "    oracion=df['text'].iloc[i]    \n",
    "    oracion = re.sub(r\"@\\S+\", \"\", oracion)  # Eliminar menciones a usuarios\n",
    "    oracion = re.sub(\"http[s]?\\://\\S+\", \"\", oracion)  # Eliminar enlaces\n",
    "    oracion = re.sub(r\"#\\S+\", \"\", oracion)  # Eliminar hashtags\n",
    "    oracion = re.sub(r\"[0-9]\", \"\", oracion)  # Eliminar n√∫meros\n",
    "    oracion = re.sub(r\"(\\(.*\\))|(\\[.*\\])\", \"\", oracion)  # Eliminar par√©ntesis y corchetes\n",
    "    oracion = re.sub(r\"\\n\", \"\", oracion)  # Eliminar caracteres de nueva l√≠nea\n",
    "    oracion = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", oracion)  # Eliminar varios patrones\n",
    "    oracion = re.sub(r\"(\\.)|(,)\", \"\", oracion)  # Eliminar puntos y comas\n",
    "    oracion = re.sub(r\"[¬°!]\", \"\", oracion)  # Eliminar signos de admiraci√≥n \n",
    "    oracion = re.sub(r\"[¬ø?]\", \"\", oracion)  # Eliminar signos de exclamaci√≥n\n",
    "    oracion = re.sub(r\"[-]\", \"\", oracion)  # Eliminar gui√≥n\n",
    "    oracion = re.sub(r\"[:\"\"¬¥¬¥''``]\", \"\", oracion)  # Eliminar otros\n",
    "    \n",
    "    #tokenizaci√≥n\n",
    "    tokens = word_tokenize(oracion)\n",
    "\n",
    "    #stopwords\n",
    "    spanish_stopwords = stopwords.words('spanish')\n",
    "    palabras_filtradas = [palabra for palabra in tokens if palabra not in spanish_stopwords]\n",
    "\n",
    "    #lematizaci√≥n\n",
    "    lema = nlp(\" \".join(palabras_filtradas))\n",
    "    oracion_lematizada = \" \".join([token.lemma_ for token in lema])\n",
    "    vectorizador = CountVectorizer()\n",
    "    vectores = vectorizador.fit_transform([oracion_lematizada])\n",
    "    vocabulario = vectorizador.get_feature_names_out()\n",
    "\n",
    "    #se guarda en listas las oraciones lematizadas y vocabulario de cada texto\n",
    "    all_voc.append(vocabulario)\n",
    "    ols.append(oracion_lematizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b13d9c-c217-433a-a447-511723f409df",
   "metadata": {},
   "source": [
    "### Vectores y vocabulario de todos los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0533299a-3883-4ac6-8ac5-5b923811cb75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acoso' 'agar' 'agresi√≥n' 'al' 'amigo' 'amodio' 'analicemos' 'aprieto'\n",
      " 'arab' 'arar' 'as√≠' 'ayuda' 'bala' 'basta' 'bueno' 'bus' 'cabeza'\n",
      " 'cadete' 'callate' 'calle' 'camello' 'cara' 'categoria' 'cerebro'\n",
      " 'chavista' 'chiste' 'chorizo' 'ciento' 'ciudadanos' 'colaborador' 'colau'\n",
      " 'conto' 'cortad' 'cosas' 'creer' 'cuatro' 'cu√°nto' 'cu√°ntos' 'c√°llate'\n",
      " 'de' 'decir' 'dejando' 'dejar' 'denunciaaaaaaar' 'des' 'detras' 'dia'\n",
      " 'donbenitense' 'durar' 'eichmann' 'el' 'elite' 'emiratos' 'en'\n",
      " 'encontrar' 'equipo' 'escolar' 'ese' 'espa√±a' 'esperar' 'esperas'\n",
      " 'estado' 'expulsarlos' 'falta' 'favor' 'femenino' 'festivit' 'frivolizar'\n",
      " 'fuenlabrada' 'futbol' 'garcho' 'gente' 'guarra' 'hacer' 'hijo' 'hoy'\n",
      " 'humillaci√≥n' 'ilegal' 'inmigracion' 'inmigrante' 'inscrito'\n",
      " 'josewifakers' 'juventud' 'kiev' 'ko' 'ladr√≥n' 'llamado' 'luchar' 'lugar'\n",
      " 'madre' 'malditar' 'mantener' 'mantero' 'marico' 'matar' 'medias' 'mejor'\n",
      " 'metete' 'mientras' 'minato' 'morir' 'moro' 'mujer' 'mundo' 'nacional'\n",
      " 'nazi' 'negratas' 'no' 'notar' 'odio' 'offshore' 'oler' 'olvidar' 'on'\n",
      " 'otro' 'pared' 'party' 'pasar' 'pa√≠s' 'peguir' 'permiso' 'pero' 'peron'\n",
      " 'perra' 'perrir' 'pie' 'pietrapierce' 'pintada' 'pobre' 'poder' 'poned'\n",
      " 'poner' 'por' 'porque' 'propio' 'protegio' 'provocaci√≥n' 'pueden' 'purs'\n",
      " 'puta' 'puto' 'p√≥n' 'que' 'querer' 'quer√©is' 'qu√©' 'rayo' 'realidad'\n",
      " 'recibir' 'recordo' 'redomicilie' 'refugiado' 'reputa' 'retirar' 'rico'\n",
      " 'sangs' 'see' 'seguro' 'semana' 'semejante' 'shame' 'shorts' 'si'\n",
      " 'sociedad' 'stars' 'story' 'subir' 'sudor' 'tal' 'tanto' 'tijerazo'\n",
      " 'tiro' 'tol' 'tonter√≠a' 'torneo' 'tres' 'turista' 'unidos' 'user'\n",
      " 'vacaci√≥n' 'verdad' 'vez' 'vida' 'viola' 'voolka' 'v√≠a' 'yogurines' 'you'\n",
      " '√°rabe' '√°rabes' '√©l']\n"
     ]
    }
   ],
   "source": [
    "# se crea un solo string de todas las oraciones lematizadas\n",
    "oraciones_lem_pegadas=' '.join(ols)\n",
    "\n",
    "#se crea el bag of words del vocabulario de todos los textos\n",
    "vectorizador_all = CountVectorizer()\n",
    "vectores_all = vectorizador_all.fit_transform([oraciones_lem_pegadas])\n",
    "vocabulario_all = vectorizador_all.get_feature_names_out()\n",
    "print(vocabulario_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c604b733-69c6-4caf-83ab-dc56cd1127a2",
   "metadata": {},
   "source": [
    "En el vocabulario global se observa que hay muchas palabras mal escritas o inexistentes en el espa√±ol. En este caso, se dejan de esta forma con tal de no modificar el proceso. No obstante, es probable que se requiera un paso extra de limpieza de datos para tener un mejor BoW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5edc6-fcd0-4774-af68-521e84ec2a2c",
   "metadata": {},
   "source": [
    "### Creaci√≥n de Dataframe con bag of words de todos los textos\n",
    "Las columnas 1 y 2 del Dataframe final corresponden al texto original y su oraci√≥n lematizada, respectivamente. \n",
    "Las columnas 2 a 191 (longitud del vocabulario de todos los textos) corresponden al vector de cada texto con valores de 0 √≥ 1, si el texto contiene una palabra del vocabulario global el valor de la celda del vector es 1, caso contrario es 0.\n",
    "La √∫ltima columna corresponde al atributo de la clase (0 o 1) del texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127b04f2-d761-4cc0-983a-4bfd3a5a75b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                texto  \\\n",
      "0   ESAS COSAS Y OTRAS PUEDEN PASAR POR MANTENER A...   \n",
      "1   28: te amodio, odio a la perra de tu amiga ‚ò∫Ô∏è‚ò∫...   \n",
      "2   @LaDivinaDiva Callate maldita perra. O seguro ...   \n",
      "3   @MarysabelPuerto Mejor callate cara de puta o ...   \n",
      "4   @xarita327 @TRIKYHUMOR @yonier2012 @casTa1326 ...   \n",
      "5               @CocotePR @ashleyhonohan callate puta   \n",
      "6   Y el inmigrante recibe ayuda del rico Estado l...   \n",
      "7   De los moros no se puede esperar nada bueno, y...   \n",
      "8   ¬øPor que si a una mujer le pegan un tiro en la...   \n",
      "9   Analicemos esto: ¬øSi te pones unos shorts as√≠,...   \n",
      "10  \"See... Tal vez les recordo como Peron protegi...   \n",
      "11  PIETRAPIERCE STORY: PURS SANGS ARABES STARS DE...   \n",
      "12  ¬øQu√© dice este de frivolizar el acoso escolar?...   \n",
      "13  #Suiza üá®üá≠ retira el permiso a 189 refugiados q...   \n",
      "14  Hoy quiero denunciaaaaaaar A LA GENTE PUTO GUA...   \n",
      "15  Redomicilie su sociedad offshore en Emiratos √Å...   \n",
      "16  @hermanntertsch Basta! Poned pie en pared a ta...   \n",
      "17  SEMANA DE LA JUVENTUD. TORNEO FUTBOL7  / FUTBO...   \n",
      "18  @lNeko_ @xTiko98 Callate y metete party de una...   \n",
      "19  Cu√°ntos inmigrantes creemos que hay, y cu√°ntos...   \n",
      "\n",
      "                                   oracion_lematizada  acoso  agar  agresi√≥n  \\\n",
      "0   ese COSAS Y otro PUEDEN pasar POR MANTENER A e...      0     0         1   \n",
      "1   amodio odio perra amigo ‚ò∫ Ô∏è ‚ò∫ Ô∏èpero vida * tij...      0     0         0   \n",
      "2   Callate malditar perra O seguro pobre marico d...      0     0         0   \n",
      "3   mejor callate cara puta reputa madre √©l notar ...      0     0         0   \n",
      "4                                        C√°llate puta      0     0         0   \n",
      "5                                        callate puto      0     0         0   \n",
      "6   y inmigrante recibir ayuda rico Estado ladr√≥n ...      0     0         0   \n",
      "7   de moro poder esperar bueno decir propio √°rabe...      0     0         0   \n",
      "8   por si mujer peguir tiro cabeza durar tres dia...      0     0         0   \n",
      "9   analicemos Si p√≥n shorts as√≠ calle esperas dec...      1     0         0   \n",
      "10  ` ` See tal vez recordo peron protegio Eichman...      0     0         0   \n",
      "11  pietrapierce STORY PURS sangs arab stars des f...      0     1         0   \n",
      "12  qu√© decir frivolizar acoso escolar Por favor q...      1     0         0   \n",
      "13    üá® üá≠ retirar permiso refugiado vacaci√≥n pa√≠s v√≠a      0     0         0   \n",
      "14  hoy querer denunciaaaaaaar A el GENTE PUTO GUA...      0     0         0   \n",
      "15  Redomicilie sociedad offshore Emiratos √Årabes ...      0     0         0   \n",
      "16  Basta Poned pie pared tanto provocaci√≥n cortad...      0     0         0   \n",
      "17  semana DE el JUVENTUD TORNEO FUTBOL / FUTBOL C...      0     0         0   \n",
      "18                      Callate metete party puta vez      0     0         0   \n",
      "19  Cu√°ntos inmigrante creer cu√°nto realidad Ciuda...      0     0         0   \n",
      "\n",
      "    al  amigo  amodio  analicemos  aprieto  ...  vida  viola  voolka  v√≠a  \\\n",
      "0    0      0       0           0        1  ...     0      0       0    1   \n",
      "1    0      1       1           0        0  ...     1      0       0    0   \n",
      "2    0      0       0           0        0  ...     0      0       0    0   \n",
      "3    0      0       0           0        0  ...     0      0       0    0   \n",
      "4    0      0       0           0        0  ...     0      0       0    0   \n",
      "5    0      0       0           0        0  ...     0      0       0    0   \n",
      "6    0      0       0           0        0  ...     0      1       0    0   \n",
      "7    0      0       0           0        0  ...     0      0       0    0   \n",
      "8    0      0       0           0        0  ...     0      0       0    0   \n",
      "9    0      0       0           1        0  ...     0      0       0    0   \n",
      "10   0      0       0           0        0  ...     0      0       0    0   \n",
      "11   0      0       0           0        0  ...     0      0       0    0   \n",
      "12   0      0       0           0        0  ...     0      0       0    0   \n",
      "13   0      0       0           0        0  ...     0      0       0    1   \n",
      "14   1      0       0           0        0  ...     0      0       0    0   \n",
      "15   0      0       0           0        0  ...     0      0       0    0   \n",
      "16   0      0       0           0        0  ...     0      0       0    0   \n",
      "17   0      0       0           0        0  ...     0      0       1    0   \n",
      "18   0      0       0           0        0  ...     0      0       0    0   \n",
      "19   0      0       0           0        0  ...     0      0       0    0   \n",
      "\n",
      "    yogurines  you  √°rabe  √°rabes  √©l  label  \n",
      "0           0    0      0       0   0      1  \n",
      "1           0    0      0       0   0      1  \n",
      "2           0    0      0       0   0      1  \n",
      "3           0    0      0       0   1      1  \n",
      "4           0    0      0       0   0      1  \n",
      "5           0    0      0       0   0      1  \n",
      "6           0    0      0       0   0      1  \n",
      "7           0    0      1       0   0      1  \n",
      "8           0    0      0       0   1      1  \n",
      "9           0    0      0       0   0      1  \n",
      "10          0    0      0       0   0      0  \n",
      "11          0    0      0       0   0      0  \n",
      "12          0    0      0       0   0      0  \n",
      "13          0    0      0       0   0      0  \n",
      "14          0    1      0       0   1      0  \n",
      "15          0    0      0       1   0      0  \n",
      "16          0    0      0       0   0      0  \n",
      "17          1    0      0       0   0      0  \n",
      "18          0    0      0       0   0      0  \n",
      "19          0    0      0       0   0      0  \n",
      "\n",
      "[20 rows x 194 columns]\n"
     ]
    }
   ],
   "source": [
    "# Se crea matriz de ceros con dimensiones de n√∫mero de textos x n√∫mero de palabras del vocabulario global\n",
    "final_vectors=np.zeros((df.shape[0],vocabulario_all.shape[0]),dtype=int)\n",
    "\n",
    "# se sustituyen con 1 los indices donde se encuentra la palabra en el arreglo del vocabulario global\n",
    "match_mask=[]\n",
    "for ind,v in enumerate(all_voc):\n",
    "    indexes=np.where(np.isin(vocabulario_all,v))[0]\n",
    "    final_vectors[ind][indexes]=1\n",
    "\n",
    "# se crea el dataframe final con los vectores por texto (cada fila corresponde a un texto)\n",
    "df_bw = pd.DataFrame(final_vectors,columns = vocabulario_all)\n",
    "#se inserta texto original\n",
    "df_bw.insert(0,'texto',df['text'].values)\n",
    "#se inserta oracion lematizada de cada texto\n",
    "df_bw.insert(1,'oracion_lematizada', ols)\n",
    "#se inserta atributo de cada clase en ultima columna\n",
    "df_bw.insert(final_vectors.shape[1]+2,'label',df['label'].values)\n",
    "#se imprime el dataframe del BoW final\n",
    "print(df_bw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
