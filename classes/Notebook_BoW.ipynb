{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c1c07e-73a8-4f64-ba8e-8a0d81138549",
   "metadata": {},
   "source": [
    "# Tarea. Notebook BoW\n",
    "### Vladimir García Loginova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8065e-2c64-4e32-8f77-941da8fdc837",
   "metadata": {},
   "source": [
    "### Se importan paqueterías y se descargan elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109f541-eae6-4323-814a-c524cbce183e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy as spc\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nlp = spc.load(\"es_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556994c-8101-41f9-9ee1-84bd55202d56",
   "metadata": {},
   "source": [
    "### Lectura de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46588aa-b031-455f-96ba-fd52a4f21840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"df_mini_HS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361e070-f1f7-41ca-bc85-b2f702c0e617",
   "metadata": {},
   "source": [
    "### Creación de listas con oraciones lematizadas y vocabulario de cada texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de33a13-22fb-43d7-807d-96ec6d48c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_voc=[]\n",
    "ols=[]\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "\n",
    "    #linmpieza de datos\n",
    "    oracion=df['text'].iloc[i]    \n",
    "    oracion = re.sub(r\"@\\S+\", \"\", oracion)  # Eliminar menciones a usuarios\n",
    "    oracion = re.sub(\"http[s]?\\://\\S+\", \"\", oracion)  # Eliminar enlaces\n",
    "    oracion = re.sub(r\"#\\S+\", \"\", oracion)  # Eliminar hashtags\n",
    "    oracion = re.sub(r\"[0-9]\", \"\", oracion)  # Eliminar números\n",
    "    oracion = re.sub(r\"(\\(.*\\))|(\\[.*\\])\", \"\", oracion)  # Eliminar paréntesis y corchetes\n",
    "    oracion = re.sub(r\"\\n\", \"\", oracion)  # Eliminar caracteres de nueva línea\n",
    "    oracion = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", oracion)  # Eliminar varios patrones\n",
    "    oracion = re.sub(r\"(\\.)|(,)\", \"\", oracion)  # Eliminar puntos y comas\n",
    "    oracion = re.sub(r\"[¡!]\", \"\", oracion)  # Eliminar signos de admiración \n",
    "    oracion = re.sub(r\"[¿?]\", \"\", oracion)  # Eliminar signos de exclamación\n",
    "    oracion = re.sub(r\"[-]\", \"\", oracion)  # Eliminar guión\n",
    "    oracion = re.sub(r\"[:\"\"´´''``]\", \"\", oracion)  # Eliminar otros\n",
    "    \n",
    "    #tokenización\n",
    "    tokens = word_tokenize(oracion)\n",
    "\n",
    "    #stopwords\n",
    "    spanish_stopwords = stopwords.words('spanish')\n",
    "    palabras_filtradas = [palabra for palabra in tokens if palabra not in spanish_stopwords]\n",
    "\n",
    "    #lematización\n",
    "    lema = nlp(\" \".join(palabras_filtradas))\n",
    "    oracion_lematizada = \" \".join([token.lemma_ for token in lema])\n",
    "    vectorizador = CountVectorizer()\n",
    "    vectores = vectorizador.fit_transform([oracion_lematizada])\n",
    "    vocabulario = vectorizador.get_feature_names_out()\n",
    "\n",
    "    #se guarda en listas las oraciones lematizadas y vocabulario de cada texto\n",
    "    all_voc.append(vocabulario)\n",
    "    ols.append(oracion_lematizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b13d9c-c217-433a-a447-511723f409df",
   "metadata": {},
   "source": [
    "### Vectores y vocabulario de todos los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533299a-3883-4ac6-8ac5-5b923811cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea un solo string de todas las oraciones lematizadas\n",
    "oraciones_lem_pegadas=' '.join(ols)\n",
    "\n",
    "#se crea el bag of words del vocabulario de todos los textos\n",
    "vectorizador_all = CountVectorizer()\n",
    "vectores_all = vectorizador_all.fit_transform([oraciones_lem_pegadas])\n",
    "vocabulario_all = vectorizador_all.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5edc6-fcd0-4774-af68-521e84ec2a2c",
   "metadata": {},
   "source": [
    "### Creación de Dataframe con bag of words de todos los textos\n",
    "Las columnas 1 y 2 del Dataframe final corresponden al texto original y su oración lematizada, respectivamente. \n",
    "Las columnas 2 a 191 (longitud del vocabulario de todos los textos) corresponden al vector de cada texto con valores de 0 ó 1, si el texto contiene una palabra del vocabulario global el valor de la celda del vector es 1, caso contrario es 0.\n",
    "La última columna corresponde al atributo de la clase (0 o 1) del texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b04f2-d761-4cc0-983a-4bfd3a5a75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea matriz de ceros con dimensiones de número de textos x número de palabras del vocabulario global\n",
    "final_vectors=np.zeros((df.shape[0],vocabulario_all.shape[0]),dtype=int)\n",
    "\n",
    "# se sustituyen con 1 los indices donde se encuentra la palabra en el arreglo del vocabulario global\n",
    "match_mask=[]\n",
    "for ind,v in enumerate(all_voc):\n",
    "    indexes=np.where(np.isin(vocabulario_all,v))[0]\n",
    "    final_vectors[ind][indexes]=1\n",
    "\n",
    "# se crea el dataframe final con los vectores por texto (cada fila corresponde a un texto)\n",
    "df_bw = pd.DataFrame(final_vectors,columns = vocabulario_all)\n",
    "#se inserta texto original\n",
    "df_bw.insert(0,'texto',df['text'].values)\n",
    "#se inserta oracion lematizada de cada texto\n",
    "df_bw.insert(1,'oracion_lematizada', ols)\n",
    "#se inserta atributo de cada clase en ultima columna\n",
    "df_bw.insert(final_vectors.shape[1]+2,'label',df['label'].values)\n",
    "#se imprime el dataframe del BoW final\n",
    "print(df_bw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
