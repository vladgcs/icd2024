{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c1c07e-73a8-4f64-ba8e-8a0d81138549",
   "metadata": {},
   "source": [
    "# Tarea. Notebook BoW\n",
    "### Vladimir Garc√≠a Loginova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8065e-2c64-4e32-8f77-941da8fdc837",
   "metadata": {},
   "source": [
    "### Se importan paqueter√≠as y se descargan elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0109f541-eae6-4323-814a-c524cbce183e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vcx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/vcx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy as spc\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nlp = spc.load(\"es_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556994c-8101-41f9-9ee1-84bd55202d56",
   "metadata": {},
   "source": [
    "### Lectura de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46588aa-b031-455f-96ba-fd52a4f21840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"df_mini_HS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361e070-f1f7-41ca-bc85-b2f702c0e617",
   "metadata": {},
   "source": [
    "### Creaci√≥n de listas con oraciones lematizadas y vocabulario de cada texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de33a13-22fb-43d7-807d-96ec6d48c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\:'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\:'\n",
      "/var/folders/fw/lfwqgrwx72v8gqhx3vm2nv780000gn/T/ipykernel_1051/3838631197.py:9: SyntaxWarning: invalid escape sequence '\\:'\n",
      "  oracion = re.sub(\"http[s]?\\://\\S+\", \"\", oracion)  # Eliminar enlaces\n"
     ]
    }
   ],
   "source": [
    "all_voc=[]\n",
    "ols=[]\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "\n",
    "    #linmpieza de datos\n",
    "    oracion=df['text'].iloc[i]    \n",
    "    oracion = re.sub(r\"@\\S+\", \"\", oracion)  # Eliminar menciones a usuarios\n",
    "    oracion = re.sub(\"http[s]?\\://\\S+\", \"\", oracion)  # Eliminar enlaces\n",
    "    oracion = re.sub(r\"#\\S+\", \"\", oracion)  # Eliminar hashtags\n",
    "    oracion = re.sub(r\"[0-9]\", \"\", oracion)  # Eliminar n√∫meros\n",
    "    oracion = re.sub(r\"(\\(.*\\))|(\\[.*\\])\", \"\", oracion)  # Eliminar par√©ntesis y corchetes\n",
    "    oracion = re.sub(r\"\\n\", \"\", oracion)  # Eliminar caracteres de nueva l√≠nea\n",
    "    oracion = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", oracion)  # Eliminar varios patrones\n",
    "    oracion = re.sub(r\"(\\.)|(,)\", \"\", oracion)  # Eliminar puntos y comas\n",
    "    oracion = re.sub(r\"[¬°!]\", \"\", oracion)  # Eliminar signos de admiraci√≥n \n",
    "    oracion = re.sub(r\"[¬ø?]\", \"\", oracion)  # Eliminar signos de exclamaci√≥n\n",
    "    oracion = re.sub(r\"[-]\", \"\", oracion)  # Eliminar guion\n",
    "    oracion = re.sub(r\"[:\"\"¬¥¬¥''``]\", \"\", oracion)  # Eliminar otros\n",
    "    \n",
    "    #tokenizaci√≥n\n",
    "    tokens = word_tokenize(oracion)\n",
    "\n",
    "    #stopwords\n",
    "    spanish_stopwords = stopwords.words('spanish')\n",
    "    palabras_filtradas = [palabra for palabra in tokens if palabra not in spanish_stopwords]\n",
    "\n",
    "    #lematizaci√≥n\n",
    "    lema = nlp(\" \".join(palabras_filtradas))\n",
    "    oracion_lematizada = \" \".join([token.lemma_ for token in lema])\n",
    "    vectorizador = CountVectorizer()\n",
    "    vectores = vectorizador.fit_transform([oracion_lematizada])\n",
    "    vocabulario = vectorizador.get_feature_names_out()\n",
    "\n",
    "    #se guarda en listas las oraciones lematizadas y vocabulario de cada texto\n",
    "    all_voc.append(vocabulario)\n",
    "    ols.append(oracion_lematizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b13d9c-c217-433a-a447-511723f409df",
   "metadata": {},
   "source": [
    "### Vectores y vocabulario de todos los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0533299a-3883-4ac6-8ac5-5b923811cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea un solo string de todas las oraciones lematizadas\n",
    "oraciones_lem_pegadas=' '.join(ols)\n",
    "\n",
    "#se crea el bag of words del vocabulario de todos los textos\n",
    "vectorizador_all = CountVectorizer()\n",
    "vectores_all = vectorizador_all.fit_transform([oraciones_lem_pegadas])\n",
    "vocabulario_all = vectorizador_all.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5edc6-fcd0-4774-af68-521e84ec2a2c",
   "metadata": {},
   "source": [
    "### Creaci√≥n de Dataframe con bag of words de todos los textos\n",
    "Las columnas 1 y 2 del Dataframe final corresponden al texto original y su oraci√≥n lematizada, respectivamente. \n",
    "Las columnas 2 a 191 (longitud del vocabulario de todos los textos) corresponden al vector de cada texto con valores de 0 √≥ 1, si el texto contiene una palabra del vocabulario global el valor de la celda del vector es 1, caso contrario es 0.\n",
    "La √∫ltima columna corresponde al atributo de la clase (0 o 1) del texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127b04f2-d761-4cc0-983a-4bfd3a5a75b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                texto  \\\n",
      "0   ESAS COSAS Y OTRAS PUEDEN PASAR POR MANTENER A...   \n",
      "1   28: te amodio, odio a la perra de tu amiga ‚ò∫Ô∏è‚ò∫...   \n",
      "2   @LaDivinaDiva Callate maldita perra. O seguro ...   \n",
      "3   @MarysabelPuerto Mejor callate cara de puta o ...   \n",
      "4   @xarita327 @TRIKYHUMOR @yonier2012 @casTa1326 ...   \n",
      "5               @CocotePR @ashleyhonohan callate puta   \n",
      "6   Y el inmigrante recibe ayuda del rico Estado l...   \n",
      "7   De los moros no se puede esperar nada bueno, y...   \n",
      "8   ¬øPor que si a una mujer le pegan un tiro en la...   \n",
      "9   Analicemos esto: ¬øSi te pones unos shorts as√≠,...   \n",
      "10  \"See... Tal vez les recordo como Peron protegi...   \n",
      "11  PIETRAPIERCE STORY: PURS SANGS ARABES STARS DE...   \n",
      "12  ¬øQu√© dice este de frivolizar el acoso escolar?...   \n",
      "13  #Suiza üá®üá≠ retira el permiso a 189 refugiados q...   \n",
      "14  Hoy quiero denunciaaaaaaar A LA GENTE PUTO GUA...   \n",
      "15  Redomicilie su sociedad offshore en Emiratos √Å...   \n",
      "16  @hermanntertsch Basta! Poned pie en pared a ta...   \n",
      "17  SEMANA DE LA JUVENTUD. TORNEO FUTBOL7  / FUTBO...   \n",
      "18  @lNeko_ @xTiko98 Callate y metete party de una...   \n",
      "19  Cu√°ntos inmigrantes creemos que hay, y cu√°ntos...   \n",
      "\n",
      "                                   oracion_lematizada  acoso  agar  agresi√≥n  \\\n",
      "0   ese COSAS Y otro PUEDEN pasar POR MANTENER A e...      0     0         1   \n",
      "1   amodio odio perra amigo ‚ò∫ Ô∏è ‚ò∫ Ô∏èpero vida * tij...      0     0         0   \n",
      "2   Callate malditar perra O seguro pobre marico d...      0     0         0   \n",
      "3   mejor callate cara puta reputa madre √©l notar ...      0     0         0   \n",
      "4                                        C√°llate puta      0     0         0   \n",
      "5                                        callate puto      0     0         0   \n",
      "6   y inmigrante recibir ayuda rico Estado ladr√≥n ...      0     0         0   \n",
      "7   de moro poder esperar bueno decir propio √°rabe...      0     0         0   \n",
      "8   por si mujer peguir tiro cabeza durar tres dia...      0     0         0   \n",
      "9   analicemos Si p√≥n shorts as√≠ calle esperas dec...      1     0         0   \n",
      "10  ` ` See tal vez recordo peron protegio Eichman...      0     0         0   \n",
      "11  pietrapierce STORY PURS sangs arab stars des f...      0     1         0   \n",
      "12  qu√© decir frivolizar acoso escolar Por favor q...      1     0         0   \n",
      "13    üá® üá≠ retirar permiso refugiado vacaci√≥n pa√≠s v√≠a      0     0         0   \n",
      "14  hoy querer denunciaaaaaaar A el GENTE PUTO GUA...      0     0         0   \n",
      "15  Redomicilie sociedad offshore Emiratos √Årabes ...      0     0         0   \n",
      "16  Basta Poned pie pared tanto provocaci√≥n cortad...      0     0         0   \n",
      "17  semana DE el JUVENTUD TORNEO FUTBOL / FUTBOL C...      0     0         0   \n",
      "18                      Callate metete party puta vez      0     0         0   \n",
      "19  Cu√°ntos inmigrante creer cu√°nto realidad Ciuda...      0     0         0   \n",
      "\n",
      "    al  amigo  amodio  analicemos  aprieto  ...  vida  viola  voolka  v√≠a  \\\n",
      "0    0      0       0           0        1  ...     0      0       0    1   \n",
      "1    0      1       1           0        0  ...     1      0       0    0   \n",
      "2    0      0       0           0        0  ...     0      0       0    0   \n",
      "3    0      0       0           0        0  ...     0      0       0    0   \n",
      "4    0      0       0           0        0  ...     0      0       0    0   \n",
      "5    0      0       0           0        0  ...     0      0       0    0   \n",
      "6    0      0       0           0        0  ...     0      1       0    0   \n",
      "7    0      0       0           0        0  ...     0      0       0    0   \n",
      "8    0      0       0           0        0  ...     0      0       0    0   \n",
      "9    0      0       0           1        0  ...     0      0       0    0   \n",
      "10   0      0       0           0        0  ...     0      0       0    0   \n",
      "11   0      0       0           0        0  ...     0      0       0    0   \n",
      "12   0      0       0           0        0  ...     0      0       0    0   \n",
      "13   0      0       0           0        0  ...     0      0       0    1   \n",
      "14   1      0       0           0        0  ...     0      0       0    0   \n",
      "15   0      0       0           0        0  ...     0      0       0    0   \n",
      "16   0      0       0           0        0  ...     0      0       0    0   \n",
      "17   0      0       0           0        0  ...     0      0       1    0   \n",
      "18   0      0       0           0        0  ...     0      0       0    0   \n",
      "19   0      0       0           0        0  ...     0      0       0    0   \n",
      "\n",
      "    yogurines  you  √°rabe  √°rabes  √©l  label  \n",
      "0           0    0      0       0   0      1  \n",
      "1           0    0      0       0   0      1  \n",
      "2           0    0      0       0   0      1  \n",
      "3           0    0      0       0   1      1  \n",
      "4           0    0      0       0   0      1  \n",
      "5           0    0      0       0   0      1  \n",
      "6           0    0      0       0   0      1  \n",
      "7           0    0      1       0   0      1  \n",
      "8           0    0      0       0   1      1  \n",
      "9           0    0      0       0   0      1  \n",
      "10          0    0      0       0   0      0  \n",
      "11          0    0      0       0   0      0  \n",
      "12          0    0      0       0   0      0  \n",
      "13          0    0      0       0   0      0  \n",
      "14          0    1      0       0   1      0  \n",
      "15          0    0      0       1   0      0  \n",
      "16          0    0      0       0   0      0  \n",
      "17          1    0      0       0   0      0  \n",
      "18          0    0      0       0   0      0  \n",
      "19          0    0      0       0   0      0  \n",
      "\n",
      "[20 rows x 194 columns]\n"
     ]
    }
   ],
   "source": [
    "# Se crea matriz de ceros con dimensiones de n√∫mero de textos x n√∫mero de palabras del vocabulario global\n",
    "final_vectors=np.zeros((df.shape[0],vocabulario_all.shape[0]),dtype=int)\n",
    "\n",
    "# se sustituyen con 1 los indices donde se encuentra la palabra en el arreglo del vocabulario global\n",
    "match_mask=[]\n",
    "for ind,v in enumerate(all_voc):\n",
    "    indexes=np.where(np.isin(vocabulario_all,v))[0]\n",
    "    final_vectors[ind][indexes]=1\n",
    "\n",
    "# se crea el dataframe final con los vectores por texto (cada fila corresponde a un texto)\n",
    "df_bw = pd.DataFrame(final_vectors,columns = vocabulario_all)\n",
    "#se inserta texto original\n",
    "df_bw.insert(0,'texto',df['text'].values)\n",
    "#se inserta oracion lematizada de cada texto\n",
    "df_bw.insert(1,'oracion_lematizada', ols)\n",
    "#se inserta atributo de cada clase en ultima columna\n",
    "df_bw.insert(final_vectors.shape[1]+2,'label',df['label'].values)\n",
    "#se imprime el dataframe del BoW final\n",
    "print(df_bw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
